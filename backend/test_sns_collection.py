"""
SNSãƒ‡ãƒ¼ã‚¿åé›†ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å•é¡Œã®åŸå› ã‚’ç‰¹å®šã™ã‚‹ãŸã‚
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime

def test_google_news_scraping(keyword="é¹¿å…å³¶ è¦³å…‰"):
    """Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆ"""
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    
    print(f"ğŸ” ãƒ†ã‚¹ãƒˆé–‹å§‹: {keyword}")
    print(f"ğŸ“¡ URL: {url}\n")
    
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®šï¼ˆGoogleãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print("ğŸ“¥ ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        response = requests.get(url, headers=headers, timeout=10)
        print(f"âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.text)} bytes\n")
        
        # HTMLã®ä¸€éƒ¨ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        print("ğŸ“‹ HTMLã®å…ˆé ­500æ–‡å­—:")
        print(response.text[:500])
        print("\n" + "="*80 + "\n")
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        print("ğŸ” HTMLã‚’ãƒ‘ãƒ¼ã‚¹ä¸­...")
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ§˜ã€…ãªã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
        print("\nğŸ“Š ã‚»ãƒ¬ã‚¯ã‚¿ãƒ†ã‚¹ãƒˆçµæœ:")
        
        # 1. articleã‚¿ã‚°
        articles = soup.select("article")
        print(f"  - 'article' ã‚»ãƒ¬ã‚¯ã‚¿: {len(articles)}ä»¶")
        if articles:
            print(f"    æœ€åˆã®è¨˜äº‹ã®ãƒ†ã‚­ã‚¹ãƒˆ: {articles[0].text.strip()[:100]}")
        
        # 2. h3ã‚¿ã‚°ï¼ˆGoogleãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯h3ã«å«ã¾ã‚Œã‚‹ã“ã¨ãŒå¤šã„ï¼‰
        h3_tags = soup.select("h3")
        print(f"  - 'h3' ã‚»ãƒ¬ã‚¯ã‚¿: {len(h3_tags)}ä»¶")
        if h3_tags:
            print(f"    æœ€åˆã®h3ã®ãƒ†ã‚­ã‚¹ãƒˆ: {h3_tags[0].text.strip()[:100]}")
        
        # 3. aã‚¿ã‚°ï¼ˆè¨˜äº‹ãƒªãƒ³ã‚¯ï¼‰
        article_links = soup.select("a[href*='/articles/']")
        print(f"  - 'a[href*=\"/articles/\"]' ã‚»ãƒ¬ã‚¯ã‚¿: {len(article_links)}ä»¶")
        if article_links:
            print(f"    æœ€åˆã®ãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ: {article_links[0].text.strip()[:100]}")
        
        # 4. ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢ï¼ˆGoogleãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ§‹é€ ã«ä¾å­˜ï¼‰
        # Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯å‹•çš„ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã€åˆæœŸHTMLã«ã¯è¨˜äº‹ãŒå«ã¾ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹
        js_articles = soup.find_all(attrs={"jslog": True})
        print(f"  - 'jslog'å±æ€§ã‚’æŒã¤è¦ç´ : {len(js_articles)}ä»¶")
        
        # 5. ãƒ‡ãƒ¼ã‚¿å±æ€§ã§æ¤œç´¢
        data_articles = soup.select("[data-n-tid]")
        print(f"  - '[data-n-tid]' ã‚»ãƒ¬ã‚¯ã‚¿: {len(data_articles)}ä»¶")
        
        # 6. å®Ÿéš›ã«å–å¾—ã§ãã‚‹è¨˜äº‹ã‚’æ¢ã™
        print("\nğŸ“° å–å¾—å¯èƒ½ãªè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:")
        results = []
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
        selectors = [
            "article h3",
            "article a",
            "h3 a",
            "[role='article'] h3",
            "[role='article'] a"
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements[:5]:  # æœ€åˆã®5ä»¶ã®ã¿
                text = elem.text.strip()
                if text and text not in results:
                    results.append(text)
                    print(f"  - {text[:80]}")
        
        print(f"\nâœ… åˆè¨ˆ {len(results)}ä»¶ã®è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—")
        
        return results
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

if __name__ == "__main__":
    test_google_news_scraping("é¹¿å…å³¶ è¦³å…‰")

